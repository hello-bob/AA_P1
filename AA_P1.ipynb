{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f115f2a3-38a9-4391-b240-33e8d9069276",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Your model will be evaluated using two metrics: profit @ top-20, and AUC. The reasons for this is to be in line with a more realistic setting. E.g. one can image data scientists in a team arguing to use AUC and optimize for that. However, as seen in the course, for this scenario, we also imagine management arguing that there is not enough budget (in terms of time and money) to contact a lot of people (or hand out a lot of promotions). Hence, they have come up with the following: based on the top-k would-be churners as predicted by your model, sum some proxy of \"retained profitability\" in case the customer was indeed a churner, or zero otherwise\n",
    "\n",
    "As a proxy of profitability, the feature average cost min was deemed to be a good value. Based on the size of the test set, k=20 was deemed to be a good choice. Hence, management cares about optimizing this metric\n",
    "Note that only about half of the test set is used for the \"public\" leaderboard. That means that the score you will see on the leaderboard is done using this part of the test only (you don't know which half). Later on through the semester, submissions are frozen and the resuls on the \"hidden\" part will be revealed\n",
    "\n",
    "Also, whilst you can definitely try, the goal is not to \"win\", but to help you reflect on your model's results, see how others are doing, etc.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "Some groups prefer to write their final report using Jupyter Notebook, which is fine too, as long as it is readable top-to-bottom\n",
    "\n",
    "You can use any predictive technique/approach you want, though focus on the whole process: general setup, critical thinking, and the ability to get and validate an outcome\n",
    "\n",
    "You're free to use unsupervised technique for your data exploration part, too. When you decide to build a black box model, including some interpretability techniques to explain it is a plus\n",
    "\n",
    "Any other assumptions or insights are thoughts can be included as well: the idea is to take what we've seen in class, get your hands dirty and try out what we've seen\n",
    "\n",
    "Perform a critical review of the evaluation metric chosen by management. How in line is it with AUC? What would you have picked instead? Were there particular issues with this chosen metric, in your view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e826ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f37fc-1746-44a5-aae7-235843420f05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27422043-f2b6-411a-9b7d-02a7a65c638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising\n",
    "TRAIN_SET_FRAC = 0.8\n",
    "SEED = 42\n",
    "TARGET_VAR = \"target\"\n",
    "DROP_VARS = ['Connect_Date', 'id'] # TBC\n",
    "KFOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296096",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHib urls to fetch data from\n",
    "url_train = 'https://raw.githubusercontent.com/hello-bob/AA_P1/main/data/train.csv'\n",
    "url_test = 'https://raw.githubusercontent.com/hello-bob/AA_P1/main/data/test.csv'\n",
    "\n",
    "# Read train and test data\n",
    "train_data = pd.read_csv(url_train, sep = ',', skipinitialspace = True, engine = 'python')\n",
    "train_data = train_data.drop(columns=DROP_VARS)\n",
    "test_data  = pd.read_csv(url_test, sep = ',', skipinitialspace = True, engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd79c8d",
   "metadata": {},
   "source": [
    "**Data exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4d189-2e25-4267-8bce-4ad4ca80f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "train_data.info()\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3045da-ba51-4ded-97cc-e1205333526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptives\n",
    "train_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571b1fd-3271-4143-9d46-7fa3a7e783f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data before modelling: Can quantitate and put it on the report since 4/5k samples\n",
    "# Apply on the test set. Train set is ok.\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "train_data.isnull().any().sort_values(ascending=False) # Columns with missing values: Dropped_calls_ratio, Usage_Band, call_cost_per_min.\n",
    "train_data[train_data.isnull().any(axis=1)] # 4 cases, 2 churners\n",
    "\n",
    "imputer_compiled = ColumnTransformer(\n",
    "    [(\"numeric_imputer\", SimpleImputer(strategy=\"median\",), [\"Dropped_calls_ratio\", \"call_cost_per_min\"]),\n",
    "     (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\"), [\"Usage_Band\"])]\n",
    ")\n",
    "\n",
    "# Imput median for numeric variables first. Because \"most_frequent\" strategy will impute for both numeric and categorical data\n",
    "train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\", \"Usage_Band\"]] = imputer_compiled.fit_transform(train_data)\n",
    "test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\", \"Usage_Band\"]] = imputer_compiled.transform(test_data)\n",
    "\n",
    "# Correcting dtype\n",
    "train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]] = train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]].astype(float)\n",
    "test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]] = test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca331e3-1828-47d3-95ef-54b2c13b6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] Pie chart about class inbalance (train set) + Percentage churn in categorical variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] correlation plot\n",
    "corr = train_data.corr(numeric_only=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = corr.columns,\n",
    "        y = corr.index,\n",
    "        z = np.array(corr),\n",
    "        text=corr.values,\n",
    "        texttemplate='%{text:.2f}'\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f38a9-01d7-49f8-a632-94777c042378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] Correlation between categorical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062db3d",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "# outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "outlier_df = (train_data.select_dtypes(include='number')\n",
    "              .drop(columns=TARGET_VAR)\n",
    "              .dropna()\n",
    "              .copy())\n",
    "\n",
    "iso_forest = IsolationForest(random_state=SEED, n_jobs=-1, contamination=0.05).fit(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ae922",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['outlier_score'] = iso_forest.decision_function(outlier_df) # more negative indicates higher outlier-ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d28176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] correlation plot\n",
    "corr = outlier_df.corr(numeric_only=True)\n",
    "corr['outlier_score'].sort_values(ascending=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top 10 outlier_scores vs the other\n",
    "top_10_outliers_idx = outlier_df.sort_values(by='outlier_score', ascending=True).head(10).index\n",
    "top_10_outliers = outlier_df[outlier_df.index.isin(top_10_outliers_idx)].copy()\n",
    "not_top_10_outliers = outlier_df[~outlier_df.index.isin(top_10_outliers_idx)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97952fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing \n",
    "top_10_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb14354",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_top_10_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836b39b-91db-4b0e-9391-a092407db57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/404017/how-to-get-top-features-that-contribute-to-anomalies-in-isolation-forest\n",
    "\"\"\"\n",
    "Outstanding variables contributing to outlier (to the left of 0 on x axis) are AveOffPeak, average cost min, AveWeekend, AveNational and \n",
    "Dropped_calls_ratio. These tend to indicate the higher the values, the more of an outlier they are.\n",
    "\"\"\"\n",
    "import shap\n",
    "\n",
    "# Create shap values and plot them\n",
    "shap_values = shap.TreeExplainer(iso_forest).shap_values(outlier_df)\n",
    "shap.summary_plot(shap_values, outlier_df, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a7f3c-d87e-4042-a0dc-e97f6c9c2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 of the top 10 outliers are churners. No discernible pattern between churn and non-churn. Propose to keep all and compare \n",
    "# between models which are robust against outliers, and those not.\n",
    "outlier_df[TARGET_VAR] = train_data[TARGET_VAR].copy()\n",
    "outlier_df.sort_values(by='outlier_score', ascending=True).head(10).sort_values(by='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6a394-39ae-44e0-be8a-c4e289fe5111",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c6f2d-0b28-40c0-88f9-8abcd84bf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=TARGET_VAR)\n",
    "y = train_data[TARGET_VAR] \n",
    "\n",
    "NUM_VARS = train_data.select_dtypes(include='number').drop(columns=TARGET_VAR).columns\n",
    "CAT_VARS = train_data.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f56a44-d58a-4cb4-b9ff-f3a54d71bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Define preprocessors for numerical and categorical features\n",
    "numerical_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abc0a1-483f-42d5-a15f-679200a52eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessors and model\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer([\n",
    "        (\"numerical\", numerical_preprocessor, NUM_VARS),\n",
    "        (\"categorical\", categorical_preprocessor, CAT_VARS)\n",
    "    ])),\n",
    "    (\"model\", SVC(probability=True, random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c74dbe-c4a1-4197-83b8-2e1a4607933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVM\n",
    "parameters = {'model__kernel':['linear', 'rbf'], \n",
    "              'model__C':[1]} # rmb to add the double underscores to allow gridsearch to fit on pipelines\n",
    "svc_gs_est = GridSearchCV(estimator=model, param_grid=parameters,cv=KFOLD,\n",
    "                      scoring=\"roc_auc\",n_jobs=-1, refit=True)\n",
    "svc_gs_est.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4184d7c-0bf5-439f-8a0b-9f4ce5f775cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs_results = pd.DataFrame(data=svc_gs_est.cv_results_)\n",
    "svc_gs_results.sort_values(by='rank_test_score', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3bd00-ce25-485d-972b-ebc36b5b7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef09650-f231-4b8e-bda4-b47eacce506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc532f4-06da-4a38-aafb-dd9a7f7b32d4",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b1c70-918e-4389-8e65-d30a3917ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best model: Set up the params accordingly\n",
    "numerical_preprocessor = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\"))\n",
    "])\n",
    "\n",
    "best_model = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer([\n",
    "        (\"numerical\", numerical_preprocessor, NUM_VARS),\n",
    "        (\"categorical\", categorical_preprocessor, CAT_VARS)\n",
    "    ])),\n",
    "    (\"model\", SVC(probability=True, random_state=SEED, C=1, kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "\n",
    "best_model.fit(X, y)\n",
    "pred = pd.DataFrame(best_model.predict_proba(test_data), \n",
    "                    columns=[\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e38898-915b-46a7-8309-d62194e9bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333172b",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ed4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41843607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing\n",
    "numeric_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, NUM_VARS),\n",
    "        (\"cat\", categorical_transformer, CAT_VARS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "# Alternatively split train-test before, do preprocessing on training data (fit_transform) then transform test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X_preprocessed, y,\n",
    "test_size=0.2, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffc172",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.2, 0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_medium = {  'n_estimators': np.arange(80, 120, 1)\n",
    "                         , 'max_depth': range(8, 16)\n",
    "                         , 'learning_rate': np.arange(0.05, 5.05, 0.05)\n",
    "                         , 'subsample': [0.6, 0.8, 1]\n",
    "                         , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                         , 'reg_lambda': [0, 1, 2, 5]\n",
    "                         }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_medium\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got quite different reults, but different hyperparameter combinations can give similar results.\n",
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [90, 100, 110, 120]\n",
    "                  , 'max_depth': [8, 10, 12]\n",
    "                  , 'learning_rate': [0.1, 0.15, 0.2, 0.25]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "                  , 'reg_lambda': [0, 1, 2]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set\n",
    "# Test AUC is close to the \"best\" model AUC on the cross-validated training set which is a good indication of not suffering from overfitting\n",
    "predicted_probabilities = grid_auc.predict_proba(X_test)\n",
    "auc_score = roc_auc_score(y_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3693840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost model\", grid_auc.best_estimator_)\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X, y)\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('xgboost_pred.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
