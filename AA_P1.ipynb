{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f115f2a3-38a9-4391-b240-33e8d9069276",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Your model will be evaluated using two metrics: profit @ top-20, and AUC. The reasons for this is to be in line with a more realistic setting. E.g. one can image data scientists in a team arguing to use AUC and optimize for that. However, as seen in the course, for this scenario, we also imagine management arguing that there is not enough budget (in terms of time and money) to contact a lot of people (or hand out a lot of promotions). Hence, they have come up with the following: based on the top-k would-be churners as predicted by your model, sum some proxy of \"retained profitability\" in case the customer was indeed a churner, or zero otherwise\n",
    "\n",
    "As a proxy of profitability, the feature average cost min was deemed to be a good value. Based on the size of the test set, k=20 was deemed to be a good choice. Hence, management cares about optimizing this metric\n",
    "Note that only about half of the test set is used for the \"public\" leaderboard. That means that the score you will see on the leaderboard is done using this part of the test only (you don't know which half). Later on through the semester, submissions are frozen and the resuls on the \"hidden\" part will be revealed\n",
    "\n",
    "Also, whilst you can definitely try, the goal is not to \"win\", but to help you reflect on your model's results, see how others are doing, etc.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "Some groups prefer to write their final report using Jupyter Notebook, which is fine too, as long as it is readable top-to-bottom\n",
    "\n",
    "You can use any predictive technique/approach you want, though focus on the whole process: general setup, critical thinking, and the ability to get and validate an outcome\n",
    "\n",
    "You're free to use unsupervised technique for your data exploration part, too. When you decide to build a black box model, including some interpretability techniques to explain it is a plus\n",
    "\n",
    "Any other assumptions or insights are thoughts can be included as well: the idea is to take what we've seen in class, get your hands dirty and try out what we've seen\n",
    "\n",
    "Perform a critical review of the evaluation metric chosen by management. How in line is it with AUC? What would you have picked instead? Were there particular issues with this chosen metric, in your view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e826ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f37fc-1746-44a5-aae7-235843420f05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27422043-f2b6-411a-9b7d-02a7a65c638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising\n",
    "TRAIN_SET_FRAC = 0.8\n",
    "SEED = 42\n",
    "TARGET_VAR = \"target\"\n",
    "DROP_VARS = ['Connect_Date', 'id'] # TBC\n",
    "KFOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296096",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHib urls to fetch data from\n",
    "url_train = 'https://raw.githubusercontent.com/hello-bob/AA_P1/main/data/train.csv'\n",
    "url_test = 'https://raw.githubusercontent.com/hello-bob/AA_P1/main/data/test.csv'\n",
    "\n",
    "# Read train and test data\n",
    "train_data = pd.read_csv(url_train, sep = ',', skipinitialspace = True, engine = 'python')\n",
    "train_data = train_data.drop(columns=DROP_VARS)\n",
    "test_data  = pd.read_csv(url_test, sep = ',', skipinitialspace = True, engine = 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd79c8d",
   "metadata": {},
   "source": [
    "**Data exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4d189-2e25-4267-8bce-4ad4ca80f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "train_data.info()\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3045da-ba51-4ded-97cc-e1205333526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptives\n",
    "train_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da0657-d11f-4ce3-b36f-d493633faea2",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571b1fd-3271-4143-9d46-7fa3a7e783f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data before modelling: Can quantitate and put it on the report since 4/5k samples\n",
    "# Apply on the test set. Train set is ok.\n",
    "\n",
    "train_data.isnull().any().sort_values(ascending=False) # Columns with missing values: Dropped_calls_ratio, Usage_Band, call_cost_per_min.\n",
    "train_data[train_data.isnull().any(axis=1)] # 4 cases, 2 churners\n",
    "\n",
    "imputer_compiled = ColumnTransformer(\n",
    "    [(\"numeric_imputer\", SimpleImputer(strategy=\"median\",), [\"Dropped_calls_ratio\", \"call_cost_per_min\"]),\n",
    "     (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\"), [\"Usage_Band\"])]\n",
    ")\n",
    "\n",
    "# Imput median for numeric variables first. Because \"most_frequent\" strategy will impute for both numeric and categorical data\n",
    "train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\", \"Usage_Band\"]] = imputer_compiled.fit_transform(train_data)\n",
    "test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\", \"Usage_Band\"]] = imputer_compiled.transform(test_data)\n",
    "\n",
    "# Correcting dtype\n",
    "train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]] = train_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]].astype(float)\n",
    "test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]] = test_data[[\"Dropped_calls_ratio\", \"call_cost_per_min\"]].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1ac68-9bbd-4282-90b8-89d9e7451442",
   "metadata": {},
   "source": [
    "**Exploratory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca331e3-1828-47d3-95ef-54b2c13b6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] Pie chart about class inbalance (train set) + Percentage churn in categorical variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] correlation plot\n",
    "corr = train_data.corr(numeric_only=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = corr.columns,\n",
    "        y = corr.index,\n",
    "        z = np.array(corr),\n",
    "        text=corr.values,\n",
    "        texttemplate='%{text:.2f}'\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26586a96-35e3-4e1e-a139-8d8053c3bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlations which has >0.7\n",
    "\"\"\"\n",
    "All_calls_mins highly correlated to National minutes. It's a sum of National minutes + International mins maybe. This may indicate that\n",
    "majority of calls are within nation. This corresponds to how Nat_call_cost_Sum is highly correlated to actual call cost. Not sure what's the\n",
    "diff betwen actuall call cost and total call cost. Nat_call_cost_Sum could be an adjustment of actual call cost, correlation 0.999.\n",
    "\n",
    "Total_call_cost most strongly correlated with International_mins_Sum and Total_Cost. This may suggest that costs are largely driven by \n",
    "international calls\n",
    "\n",
    "Not sure what total calls indicate, maybe it's cost from other telco-related services e.g. broadband, cable tv etc.\n",
    "\n",
    "Peak_mins_Sum highly correlated with all_calls_mins and national mins. This may make sense since the more minutes of calling, the higher\n",
    "likelihood of calling during the peak period?\n",
    "\n",
    "\"\"\"\n",
    "corr_long = train_data.corr(numeric_only=True).melt(ignore_index=False).reset_index(drop=False)\n",
    "corr_long = corr_long[(corr_long['index'] != corr_long['variable']) & (abs(corr_long['value']) >0.7)]\n",
    "corr_long.sort_values(by=['index', 'variable'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f40c4-a056-4419-a83a-bf703705d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(round((train_data['International_mins_Sum'] + train_data['National mins']),2) == round(train_data['All_calls_mins'], 2)).sum() #almost all\n",
    "train_data[['All_calls_mins', 'National mins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72945191-8232-4df0-8622-4aebdc45b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f38a9-01d7-49f8-a632-94777c042378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [For report] Correlation between categorical variables, using cramer's V\n",
    "# Generally, \n",
    "from scipy.stats.contingency import association\n",
    "\n",
    "cat_corr_cols = train_data.select_dtypes('object').columns.to_list()\n",
    "while len(cat_corr_cols) > 0:\n",
    "    col = cat_corr_cols.pop(0)\n",
    "    print(f'Correlation for {col}')\n",
    "    for other_col in cat_corr_cols:\n",
    "        contingency_tbl = pd.crosstab(train_data[col], train_data[other_col])\n",
    "        cramer_V = association(contingency_tbl, method=\"cramer\")\n",
    "        print(f'Association with {other_col}: {cramer_V}')\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23673c-392c-4d38-8155-8aaef96af080",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_data['tariff'], train_data['Usage_Band'])\n",
    "pd.crosstab(train_data['Gender'], train_data['Handset'])\n",
    "pd.crosstab(train_data['high Dropped calls'], train_data['Handset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f838843-72c5-42f5-bb50-956761647903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the metric by management\n",
    "train_data.sort_values(by='average cost min', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying outliers via isolation forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "outlier_df = (train_data.select_dtypes(include='number')\n",
    "              .drop(columns=TARGET_VAR)\n",
    "              .dropna()\n",
    "              .copy())\n",
    "\n",
    "iso_forest = IsolationForest(random_state=SEED, n_jobs=-1, contamination=0.05).fit(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ae922",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['outlier_score'] = iso_forest.decision_function(outlier_df) # more negative indicates higher outlier-ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836b39b-91db-4b0e-9391-a092407db57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/404017/how-to-get-top-features-that-contribute-to-anomalies-in-isolation-forest\n",
    "\"\"\"\n",
    "Outstanding variables contributing to outlier (to the left of 0 on x axis) are AveOffPeak, average cost min, AveWeekend, AveNational and \n",
    "Dropped_calls_ratio. These tend to indicate the higher the values, the more of an outlier they are.\n",
    "\"\"\"\n",
    "\n",
    "# Create shap values and plot them\n",
    "shap_values = shap.TreeExplainer(iso_forest).shap_values(outlier_df)\n",
    "shap.summary_plot(shap_values, outlier_df, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3969e6e-3415-40aa-9727-4b6fbb5577db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shap per variable Top few: Weekend_calls_sum, nat_call_cost_sum, dropped_calls, average cost min\n",
    "# a global measure of feature importance (https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)\n",
    "# These values seem low to the values I saw online. \n",
    "explainer = shap.Explainer(iso_forest, outlier_df)\n",
    "shap_values = explainer(outlier_df)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a7f3c-d87e-4042-a0dc-e97f6c9c2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 of the top 10 outliers are churners. No discernible pattern between churn and non-churn. Propose to keep all and compare \n",
    "# between models which are robust against outliers, and those not.\n",
    "outlier_df[TARGET_VAR] = train_data[TARGET_VAR].copy()\n",
    "outlier_df.sort_values(by='outlier_score', ascending=True).head(10).sort_values(by='target')\n",
    "outlier_df.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6a394-39ae-44e0-be8a-c4e289fe5111",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c6f2d-0b28-40c0-88f9-8abcd84bf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=TARGET_VAR)\n",
    "y = train_data[TARGET_VAR] \n",
    "\n",
    "NUM_VARS = train_data.select_dtypes(include='number').drop(columns=TARGET_VAR).columns\n",
    "CAT_VARS = train_data.select_dtypes(include='object').columns\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=1/KFOLD, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60157d-b88a-48b7-ae93-592646fac04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NUM_VARS)\n",
    "print(CAT_VARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f56a44-d58a-4cb4-b9ff-f3a54d71bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessors for numerical and categorical features\n",
    "numerical_preprocessor = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler(clip=True))\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abc0a1-483f-42d5-a15f-679200a52eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessors and model\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer([\n",
    "        (\"numerical\", numerical_preprocessor, NUM_VARS),\n",
    "        (\"categorical\", categorical_preprocessor, CAT_VARS)\n",
    "    ])),\n",
    "    (\"model\", SVC(probability=True, random_state=SEED, max_iter=25000))\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c74dbe-c4a1-4197-83b8-2e1a4607933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For SVM\n",
    "\n",
    "parameters = {'model__kernel':['linear', 'rbf', 'sigmoid'], \n",
    "              'model__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "              'model__gamma':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "             'model__class_weight':[None, 'balanced']} # rmb to add the double underscores to allow gridsearch to fit on pipelines\n",
    "svc_gs_est = GridSearchCV(estimator=model, param_grid=parameters,cv=KFOLD,\n",
    "                      scoring=\"roc_auc\",n_jobs=-1, refit=True, verbose=10)\n",
    "svc_gs_est.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4184d7c-0bf5-439f-8a0b-9f4ce5f775cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs_results = pd.DataFrame(data=svc_gs_est.cv_results_)\n",
    "svc_gs_results.sort_values(by='rank_test_score', ascending = True).to_csv('output/svc_gridsearchcv_minmaxScaler.csv')\n",
    "svc_gs_results.sort_values(by='rank_test_score', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf85310-6664-49e2-ade5-21e2e21765b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params\n",
    "svc_gs_est.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7158948-7b44-41a7-bb43-b6bee72f4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "\"\"\"\n",
    "Goes up fast, but then struggles to improve TPR without increasing FPR\n",
    "\"\"\"\n",
    "predicted_probabilities = svc_gs_est.predict_proba(X_test)\n",
    "auc_score = roc_auc_score(y_test, predicted_probabilities[:, 1])\n",
    "print(f\"AUC Score: {round(auc_score, 3)}\") # 0.9265646948649914\n",
    "\n",
    "RocCurveDisplay.from_estimator(svc_gs_est, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a6f6a-464c-4077-8011-a0af73560123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance: To note, some numerical variables share permutation importance due to high correlation\n",
    "# Handset, international mins sum and usage band is at the top. As above:\n",
    "\n",
    "\"\"\"\n",
    "All_calls_mins highly correlated to National minutes. It's a sum of National minutes + International mins maybe. [International and all call mins quite impt]\n",
    "Nat_call_cost_Sum could be an adjustment of actual call cost, correlation 0.999. [Actual call cost much more important than nat call cost]\n",
    "Total_call_cost most strongly correlated with International_mins_Sum and Total_Cost [all 3 relatively impt]\n",
    "Peak_mins_Sum highly correlated with all_calls_mins and national mins [these 3 all low feature importance]\n",
    "\n",
    "\"\"\"\n",
    "perm_importance = permutation_importance(svc_gs_est, X_test, y_test)\n",
    "\n",
    "features = svc_gs_est.feature_names_in_\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bc02f-6b81-4e8a-a02c-203e65e8b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predicted probabilities against metric avg cost min\n",
    "# 50% seems to be a clean cut-off\n",
    "plt.scatter(y=predicted_probabilities[:, 1], x=X_test['average cost min'], alpha=0.6, )\n",
    "plt.xlabel(\"average cost min\")\n",
    "plt.ylabel(\"pred probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37505859-814c-42be-9575-664f2f2e6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting residuals against metric avg cost min\n",
    "# Most individuals where the algorithm predicts within a limit of +/-0.5 do not have high average cost per minute.\n",
    "# The high value customers were \"false positives\" in a sense, if we were to use a decision boundary of 0.5.\n",
    "plt.scatter(y=(y_test-predicted_probabilities[:, 1]), x=X_test['average cost min'], alpha=0.6)\n",
    "plt.xlabel(\"average cost min\")\n",
    "plt.ylabel(\"residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e031c-62df-452b-82bc-337861bb75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 25% as threshold \n",
    "pd.crosstab((predicted_probabilities[:, 1]>=0.25).astype(int), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b1c70-918e-4389-8e65-d30a3917ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best model on all train data: Set up the params accordingly\n",
    "numerical_preprocessor = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(drop=\"if_binary\"))\n",
    "])\n",
    "\n",
    "best_svc_model = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer([\n",
    "        (\"numerical\", numerical_preprocessor, NUM_VARS),\n",
    "        (\"categorical\", categorical_preprocessor, CAT_VARS)\n",
    "    ])),\n",
    "    (\"model\", SVC(probability=True, random_state=SEED, C=1000, gamma=0.001, class_weight='balanced', kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "best_svc_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e38898-915b-46a7-8309-d62194e9bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':best_svc_model.predict_proba(test_data)[:,1]})\n",
    "test_data_sub.to_csv('output/svc_submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ba0a9-094e-4514-b51f-935691a70fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission: Adjust for those cases with high average cost min\n",
    "test_data_sub_adjusted = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':best_svc_model.predict_proba(test_data)[:,1],\n",
    "                                           'average cost min':test_data['average cost min']})\n",
    "test_data_sub_adjusted.plot(y=\"PRED\", x=\"average cost min\", kind=\"scatter\")\n",
    "test_data_sub_adjusted['quantile'] = (pd.qcut(test_data_sub_adjusted[\"average cost min\"].values, 10,labels=False) + 1) / 10\n",
    "test_data_sub_adjusted['PRED'] = test_data_sub_adjusted['PRED'] * test_data_sub_adjusted['quantile']\n",
    "test_data_sub_adjusted['PRED'].describe()\n",
    "test_data_sub.to_csv('output/svc_submission_adjusted1.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a1442-708c-419d-afa9-f57ce92e0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission: use prediction threshold of 25%.  \n",
    "test_data_sub_hardadjusted = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':best_svc_model.predict_proba(test_data)[:,1]})\n",
    "test_data_sub_hardadjusted['PRED'] = (test_data_sub_hardadjusted['PRED'] >=0.25).astype(int)\n",
    "test_data_sub_hardadjusted.to_csv('output/svc_submission_hardadjusted.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efe02d-3066-4a9d-95d5-4f0d2d67a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission: use prediction threshold of 25%. Sort values by average cost min \n",
    "test_data_sub_hardadjusted_sorted = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':best_svc_model.predict_proba(test_data)[:,1],\n",
    "                                           'average cost min':test_data['average cost min']})\n",
    "test_data_sub_hardadjusted_sorted['PRED'] = (test_data_sub_hardadjusted_sorted['PRED'] >=0.25).astype(int)\n",
    "test_data_sub_hardadjusted_sorted = test_data_sub_hardadjusted_sorted.sort_values(by=['PRED', 'average cost min'], ascending=False)\n",
    "test_data_sub_hardadjusted_sorted = test_data_sub_hardadjusted_sorted.drop(columns=['average cost min'])\n",
    "test_data_sub_hardadjusted_sorted.to_csv('output/svc_submission_hardadjusted_sorted.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def056eb-7be3-4ef4-afcd-9c81830e8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For submission: use prediction threshold of 25% + adjust for cases with high average cosst min. \n",
    "test_data_sub_adjusted2 = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':best_svc_model.predict_proba(test_data)[:,1],\n",
    "                                           'average cost min':test_data['average cost min']})\n",
    "test_data_sub_adjusted2['PRED'] = (test_data_sub_adjusted2['PRED'] >=0.25).astype(int)\n",
    "test_data_sub_adjusted2['quantile'] = (pd.qcut(test_data_sub_adjusted2[\"average cost min\"].values, 10,labels=False) + 1) / 10\n",
    "test_data_sub_adjusted2['PRED'] = test_data_sub_adjusted2['PRED'] * test_data_sub_adjusted2['quantile'] \n",
    "\n",
    "test_data_sub_adjusted2.plot(x=\"quantile\", y=\"PRED\", kind=\"scatter\")\n",
    "test_data_sub_adjusted2 = test_data_sub_adjusted2.drop(columns=['quantile', 'average cost min'])\n",
    "test_data_sub_adjusted2.to_csv('output/svc_submission_adjusted2.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c789467",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41843607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing\n",
    "numeric_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, NUM_VARS),\n",
    "        (\"cat\", categorical_transformer, CAT_VARS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "# Alternatively split train-test before, do preprocessing on training data (fit_transform) then transform test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X_preprocessed, y, test_size=0.2, stratify=y, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffc172",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad460f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte class ratio. Will be used to assess class imbalance during training\n",
    "ratio = float(y_train.value_counts()[0]) / y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.2, 0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_medium = {  'n_estimators': np.arange(50, 120, 1)\n",
    "                         , 'max_depth': range(6, 15)\n",
    "                         , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                         , 'subsample': [0.6, 0.8, 1]\n",
    "                         , 'colsample_bytree': [0.8, 1]\n",
    "                         , 'reg_lambda': [0, 1, 2, 5]\n",
    "                         }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_medium\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got quite different reults, but different hyperparameter combinations can give similar results.\n",
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [70, 80, 100, 120]\n",
    "                  , 'max_depth': [8, 10, 12]\n",
    "                  , 'learning_rate': [0.05, 0.1, 0.15, 0.2]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.8, 1]\n",
    "                  , 'reg_lambda': [0, 1, 2]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679bd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an output based on the previous (last) grid search\n",
    "pd.DataFrame(grid_auc.cv_results_).sort_values(by='rank_test_score', ascending = True).to_csv('output/xgb_gridsearch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set\n",
    "# Test AUC is close to the \"best\" model AUC on the cross-validated training set which is a good indication of not suffering from overfitting\n",
    "predicted_probabilities = grid_auc.predict_proba(X_test)\n",
    "auc_score = roc_auc_score(y_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3693840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost model\", xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio, colsample_bytree=1, learning_rate=0.05, max_depth=12, n_estimators=100, reg_lambda=0, subsample=0.8))\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X, y)\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_pred_submission_v2.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71c176",
   "metadata": {},
   "source": [
    "Sample weights based on average cost min (?) - experimental code, can be deleted later, not sure exactly how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.2, 0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train, sample_weight=X_train[:,24])\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80825f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(20, 60, 1)\n",
    "                        , 'max_depth': range(5, 11)\n",
    "                        , 'learning_rate': np.arange(0.05, 5.05, 0.05)\n",
    "                        , 'subsample': [0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.6, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_train, y_train, sample_weight=X_train[:,24])\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [30, 50, 100]\n",
    "                  , 'max_depth': [8, 10, 12]\n",
    "                  , 'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.8, 1]\n",
    "                  , 'reg_lambda': [0, 1, 2]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_train, y_train, sample_weight=X_train[:,24])\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an output based on the previous (last) grid search\n",
    "pd.DataFrame(grid_auc.cv_results_).sort_values(by='rank_test_score', ascending = True).to_csv('output/xgb_gridsearch_weighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set\n",
    "# Test AUC is close to the \"best\" model AUC on the cross-validated training set which is a good indication of not suffering from overfitting\n",
    "predicted_probabilities = grid_auc.predict_proba(X_test)\n",
    "auc_score = roc_auc_score(y_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40833b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost_model\", grid_auc.best_estimator_)\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X, y) # , xgboost_model__sample_weight=np.array(X['average cost min'])\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_pred_submission_v3_weighted.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59651b8",
   "metadata": {},
   "source": [
    "**Random_Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the balanced Random Forest classifier\n",
    "rf_classifier = BalancedRandomForestClassifier(random_state=42, sampling_strategy=\"all\", replacement=True, bootstrap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933eea2",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a broader grid search to get the best criterions\n",
    "n_estimators=np.arange(10,200,10)\n",
    "criterion=[\"entropy\",\"gini\"]\n",
    "max_features=[\"log2\",\"sqrt\"]\n",
    "class_weight=[None, \"balanced\", \"balanced_subsample\"]\n",
    "max_depth=[None,3,7,11]\n",
    "min_samples_split=[2,3,4] \n",
    "min_samples_leaf=[1,2]\n",
    "\n",
    "param_grid={\"n_estimators\":n_estimators,\"criterion\":criterion,\"max_features\":max_features,\"class_weight\":class_weight,\"max_depth\":max_depth,\"min_samples_split\":min_samples_split,\"min_samples_leaf\":min_samples_leaf}\n",
    "\n",
    "# to get the results\n",
    "rf_grid=GridSearchCV(estimator=rf_classifier,param_grid=param_grid,cv=3,verbose=0,n_jobs=4)\n",
    "rf_grid.fit(X_train,y_train)\n",
    "\n",
    "rf_grid.fit(X_train, y_train, sample_weight=X_train[:,24])\n",
    "print(\"Best parameters found: \",rf_grid.best_params_)\n",
    "print(\"best score found: \", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do it one more time around the best parameters\n",
    "n_estimators=np.arange(110,130,1)\n",
    "\n",
    "param_grid2={\"n_estimators\":n_estimators}\n",
    "\n",
    "# All the best estimator found are the default ones\n",
    "rf_classifier2 = BalancedRandomForestClassifier(criterion=\"entropy\", random_state=42, sampling_strategy=\"all\", replacement=True, bootstrap=False)\n",
    "\n",
    "# to get the results\n",
    "rf_grid2 = GridSearchCV(estimator=rf_classifier2,param_grid=param_grid2,cv=3,verbose=0,n_jobs=4)\n",
    "rf_grid2.fit(X_train,y_train)\n",
    "\n",
    "rf_grid2.fit(X_train, y_train, sample_weight=X_train[:,24])\n",
    "print(\"Best parameters found: \",rf_grid2.best_params_)\n",
    "print(\"best score found: \", rf_grid2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb243bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we train the tuned model on the full data\n",
    "best_model_random_forest = BalancedRandomForestClassifier(n_estimators=116, criterion=\"entropy\", random_state=42, sampling_strategy=\"all\", replacement=True, bootstrap=False)\n",
    "\n",
    "best_model_random_forest.fit(X, y) # , xgboost_model__sample_weight=np.array(X['average cost min'])\n",
    "pred_random_forest = pd.DataFrame(best_model_random_forest.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub2 = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_random_forest[\"1\"]})\n",
    "test_data_sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxporting the results\n",
    "test_data_sub2.to_csv('output/balanced_random_forest_weighted_pred_submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493069c",
   "metadata": {},
   "source": [
    "**Training a seperate models on different tiers of customers (based on profitability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the density plot of 'average cost min'\n",
    "# We see that most customers have around 0.15 average cost min. Anything above 0.3 is pretty high (toptop customers), and anything above 0.2 is relatively high (top customers)\n",
    "sns.kdeplot(data=train_data, x='average cost min', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around the top 12% of customers have higher 'average cost min' than 0.2 and around the top 3.3% of customers have higher 'average cost min' than 0.3\n",
    "print(train_data['average cost min'].quantile(0.88))\n",
    "print(train_data['average cost min'].quantile(0.967))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed02c4",
   "metadata": {},
   "source": [
    "**Creating 3 models**\n",
    "\n",
    "1st on customers with 'average cost min' > 0.3 ==> Gold customers\n",
    "\n",
    "2nd on customers with 0.2 < 'average cost min' <= 0.3 ==> Silver customers \n",
    "\n",
    "3rd is the rest ('average cost min' <= 0.2) ==> Bronze customers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data   = train_data[train_data['average cost min'] > 0.3]\n",
    "silver_data = train_data[(train_data['average cost min'] > 0.2) & (train_data['average cost min'] <= 0.3)]\n",
    "bronze_data = train_data[train_data['average cost min'] <= 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d686bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gold   = gold_data.drop(columns=TARGET_VAR)\n",
    "y_gold   = gold_data[TARGET_VAR]\n",
    "X_silver = silver_data.drop(columns=TARGET_VAR)\n",
    "y_silver = silver_data[TARGET_VAR]\n",
    "X_bronze = bronze_data.drop(columns=TARGET_VAR)\n",
    "y_bronze = bronze_data[TARGET_VAR]\n",
    "\n",
    "X_gold_preprocessed = preprocessor.fit_transform(X_gold)\n",
    "X_silver_preprocessed = preprocessor.fit_transform(X_silver)\n",
    "X_bronze_preprocessed = preprocessor.fit_transform(X_bronze)\n",
    "\n",
    "X_gold_train, X_gold_test, y_gold_train, y_gold_test= train_test_split(X_gold_preprocessed, y_gold, test_size=0.2, stratify=y_gold, random_state=420)\n",
    "X_silver_train, X_silver_test, y_silver_train, y_silver_test= train_test_split(X_silver_preprocessed, y_silver, test_size=0.2, stratify=y_silver, random_state=420)\n",
    "X_bronze_train, X_bronze_test, y_bronze_train, y_bronze_test= train_test_split(X_bronze_preprocessed, y_bronze, test_size=0.2, stratify=y_bronze, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef794c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte class ratio. Will be used to assess class imbalance during training\n",
    "ratio_gold = float(y_gold_train.value_counts()[0]) / y_gold_train.value_counts()[1]\n",
    "ratio_silver = float(y_silver_train.value_counts()[0]) / y_silver_train.value_counts()[1]\n",
    "ratio_bronze = float(y_bronze_train.value_counts()[0]) / y_bronze_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6baba8",
   "metadata": {},
   "source": [
    "Gold customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_gold)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_gold_train, y_gold_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search (with more random hyperpar. combinations) over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(50, 110, 1)\n",
    "                        , 'max_depth': range(5, 20)\n",
    "                        , 'learning_rate': np.arange(0.05, 5.05, 0.01)\n",
    "                        , 'subsample': [0.8, 1]\n",
    "                        , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [10, 100, 200]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_gold)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=5000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_gold_train, y_gold_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [70, 73, 74, 75, 80]\n",
    "                  , 'max_depth': [15, 18, 19, 20, 25]\n",
    "                  , 'learning_rate': [0.5, 0.6, 0.67, 1]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                  , 'reg_lambda': [100, 200, 300]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_gold)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_gold_train, y_gold_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an output based on the previous (last) grid search\n",
    "pd.DataFrame(grid_auc.cv_results_).sort_values(by='rank_test_score', ascending = True).to_csv('output/xgb_gridsearch_gold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set - on the test set of the \"top customers\"\n",
    "# Test AUC is lower than the \"best\" model AUC on the cross-validated training set, but not tragic. Also the 'gold' customer subset has the least data points.\n",
    "predicted_probabilities = grid_auc.predict_proba(X_gold_test)\n",
    "auc_score = roc_auc_score(y_gold_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost model\", xgb.XGBClassifier(  random_state=420\n",
    "                                        , scale_pos_weight=ratio_gold\n",
    "                                        , colsample_bytree=0.5\n",
    "                                        , learning_rate=0.67\n",
    "                                        , max_depth=15\n",
    "                                        , n_estimators=74\n",
    "                                        , reg_lambda=200\n",
    "                                        , subsample=1))\n",
    "                        # Could have just used grid_auc.best_estimator_, but that needs to be saved into memory and if we close the notebook we would need to rerun the\n",
    "                        # gridsearch to get it into memory again\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X_gold, y_gold)\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Appending predictions to the test data\n",
    "test_data['gold_pred'] = pred_xgb[\"1\"]\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_submission_gold.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbc7b1",
   "metadata": {},
   "source": [
    "Silver customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_silver)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_silver_train, y_silver_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search (with more random hyperpar. combinations) over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 30, 1)\n",
    "                        , 'max_depth': range(5, 20)\n",
    "                        , 'learning_rate': np.arange(0.05, 5.05, 0.01)\n",
    "                        , 'subsample': [0.8, 1]\n",
    "                        , 'colsample_bytree': [0.8, 1]\n",
    "                        , 'reg_lambda': [10, 100, 200]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_silver)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=5000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_silver_train, y_silver_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fefc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [8, 10, 15, 20]\n",
    "                  , 'max_depth': [5, 6, 7, 10, 15]\n",
    "                  , 'learning_rate': [0.5, 0.79, 1]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.8, 1]\n",
    "                  , 'reg_lambda': [50, 100, 200]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_silver)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_silver_train, y_silver_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an output based on the previous (last) grid search\n",
    "pd.DataFrame(grid_auc.cv_results_).sort_values(by='rank_test_score', ascending = True).to_csv('output/xgb_gridsearch_silver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e789b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set - on the test set of the \"top customers\"\n",
    "# Test AUC is close to the \"best\" model AUC on the cross-validated training set which is a good indication of not suffering from overfitting\n",
    "predicted_probabilities = grid_auc.predict_proba(X_silver_test)\n",
    "auc_score = roc_auc_score(y_silver_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost model\", xgb.XGBClassifier(  random_state=420\n",
    "                                        , scale_pos_weight=ratio_silver\n",
    "                                        , colsample_bytree=1\n",
    "                                        , learning_rate=0.79\n",
    "                                        , max_depth=6\n",
    "                                        , n_estimators=10\n",
    "                                        , reg_lambda=100\n",
    "                                        , subsample=0.8))\n",
    "                        # Could have just used grid_auc.best_estimator_, but that needs to be saved into memory and if we close the notebook we would need to rerun the\n",
    "                        # gridsearch to get it into memory again)\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X_silver, y_silver)\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Appending predictions to the test data\n",
    "test_data['silver_pred'] = pred_xgb[\"1\"]\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_submission_silver.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67823d56",
   "metadata": {},
   "source": [
    "Bronze customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do a random search over a large hyperparameter space, trying 1000 random models \n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(5, 101, 1)\n",
    "                        , 'max_depth': range(2, 13)\n",
    "                        , 'learning_rate': np.arange(0.001, 5, 0.01)\n",
    "                        , 'subsample': [0.6, 0.8, 1]\n",
    "                        , 'colsample_bytree': [0.5, 0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 5, 10, 100]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_bronze)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_bronze_train, y_bronze_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do another random search (with more random hyperpar. combinations) over a smaller hyperparameter space around the preivously found \"best\" values\n",
    "gbm_param_grid_large = {  'n_estimators': np.arange(15, 50, 1)\n",
    "                        , 'max_depth': range(5, 20)\n",
    "                        , 'learning_rate': np.arange(0.05, 5.05, 0.01)\n",
    "                        , 'subsample': [0.8, 1]\n",
    "                        , 'colsample_bytree': [0.8, 1]\n",
    "                        , 'reg_lambda': [0, 1, 10, 20]\n",
    "                        }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_bronze)\n",
    "randomized_auc = RandomizedSearchCV(  estimator=gbm\n",
    "                                    , param_distributions=gbm_param_grid_large\n",
    "                                    , n_iter=1000\n",
    "                                    , scoring='roc_auc'\n",
    "                                    , cv=5\n",
    "                                    , n_jobs=-1\n",
    "                                    , verbose=1\n",
    "                                    , random_state=420)\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "randomized_auc.fit(X_bronze_train, y_bronze_train)\n",
    "print(\"Best parameters found: \",randomized_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", randomized_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally a grid-search that is not random around the previous \"best\" values.\n",
    "gbm_param_grid = {  'n_estimators': [15, 20, 21, 22, 30]\n",
    "                  , 'max_depth': [5, 8, 9, 10, 15]\n",
    "                  , 'learning_rate': [0.05, 0.1, 0.13, 0.2]\n",
    "                  , 'subsample': [0.8, 1]\n",
    "                  , 'colsample_bytree': [0.8, 1]\n",
    "                  , 'reg_lambda': [0, 1, 2, 5]\n",
    "                  }\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=420, scale_pos_weight=ratio_bronze)\n",
    "grid_auc = GridSearchCV(  estimator=gbm\n",
    "                        , param_grid=gbm_param_grid\n",
    "                        , scoring='roc_auc'\n",
    "                        , cv=5\n",
    "                        , n_jobs=-1\n",
    "                        , verbose=1\n",
    "                        )\n",
    "\n",
    "# See how the cross-validation performed (on the \"training\" data) and the best tuned hyperparameter values\n",
    "grid_auc.fit(X_bronze_train, y_bronze_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Lowest AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an output based on the previous (last) grid search\n",
    "pd.DataFrame(grid_auc.cv_results_).sort_values(by='rank_test_score', ascending = True).to_csv('output/xgb_gridsearch_bronze.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06992236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea about how the model performs on the test set - on the test set of the \"top customers\"\n",
    "# Test AUC is close to the \"best\" model AUC on the cross-validated training set which is a good indication of not suffering from overfitting\n",
    "predicted_probabilities = grid_auc.predict_proba(X_bronze_test)\n",
    "auc_score = roc_auc_score(y_bronze_test, predicted_probabilities[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dddd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the tuned model on the entire training data (not just on the 80% of it)\n",
    "best_model_xgb = Pipeline([\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUM_VARS),\n",
    "            (\"cat\", categorical_transformer, CAT_VARS)\n",
    "            ]\n",
    "    )),\n",
    "    (\"xgboost model\", xgb.XGBClassifier(  random_state=420\n",
    "                                        , scale_pos_weight=ratio_bronze\n",
    "                                        , colsample_bytree=0.8\n",
    "                                        , learning_rate=0.13\n",
    "                                        , max_depth=10\n",
    "                                        , n_estimators=15\n",
    "                                        , reg_lambda=1\n",
    "                                        , subsample=0.8))\n",
    "                        # Could have just used grid_auc.best_estimator_, but that needs to be saved into memory and if we close the notebook we would need to rerun the\n",
    "                        # gridsearch to get it into memory again, and it can take 5-10 minutes\n",
    "])\n",
    "\n",
    "\n",
    "best_model_xgb.fit(X_bronze, y_bronze)\n",
    "pred_xgb = pd.DataFrame(best_model_xgb.predict_proba(test_data), columns=[\"0\", \"1\"])\n",
    "\n",
    "# Appending predictions to the test data\n",
    "test_data['bronze_pred'] = pred_xgb[\"1\"]\n",
    "\n",
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':pred_xgb[\"1\"]})\n",
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_submission_bronze.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0d861",
   "metadata": {},
   "source": [
    "Creating the final predictions from the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18122a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"operational prediction\" column\n",
    "def create_operational_pred(row):\n",
    "    if row['average cost min'] > 0.3:\n",
    "        return row['gold_pred']\n",
    "    elif 0.2 < row['average cost min'] <= 0.3:\n",
    "        return row['silver_pred']\n",
    "    else:\n",
    "        return row['bronze_pred']\n",
    "\n",
    "# Apply the function to create the new column 'operational_pred'\n",
    "test_data['operational_pred'] = test_data.apply(create_operational_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15102029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final prediction - \n",
    "# For the top 40 observations sorted by 'average cost min' descending (so the most valuable customers) where the 'operational_pred' is higher than 0.5,\n",
    "# We overwrite the prediction to 1, 0.999999, 0.999998 etc.\n",
    "# This immitates that if we predict a customer to churn with 50%+ probability, and the customer has high 'average cost min', then we will try to retain that customer. We do\n",
    "# this by artificially inflating its prediction of churn. So even if a customer had barely higher \"probability\" than 0.5 but they had a large 'average cost min', we will \n",
    "# still give a high \"probability\" (close to 1) of churning. This will help elevate our TOP20 evaluation metric.\n",
    "\n",
    "# Sort the DataFrame by 'average cost min' in descending order\n",
    "df_sorted = test_data.sort_values(by='average cost min', ascending=False)\n",
    "\n",
    "# Define a function to calculate 'final_pred' based on the specified conditions\n",
    "def create_final_pred(row):\n",
    "    if row['operational_pred'] > 0.5:\n",
    "        top_records = df_sorted[df_sorted['operational_pred'] > 0.5].head(40)\n",
    "        if row.name in top_records.index:\n",
    "            return 1 - (top_records.index.get_loc(row.name) / 1000000)  # Starting from 1 and decreasing by 0.000001 for each subsequent record\n",
    "        else:\n",
    "            return row['operational_pred']\n",
    "    else:\n",
    "        return row['operational_pred']\n",
    "\n",
    "# Apply the function to create the new column 'final_pred'\n",
    "test_data['final_pred'] = test_data.apply(create_final_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data for submission\n",
    "test_data_sub = pd.DataFrame(data={'ID':test_data['id'], \n",
    "                                   'PRED':test_data['final_pred']})\n",
    "\n",
    "# Exporting results\n",
    "test_data_sub.to_csv('output/xgboost_submission_3models.csv', header=True, index=False)\n",
    "\n",
    "test_data_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
